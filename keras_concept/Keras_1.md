## Keras_1

###  목차

#### 1. 딥러닝 작동원리

#### 2. 특성공학

#### 3. 데이터 표현



### 1. 딥러닝 작동원리

##### 1) 손실함수 - 1.신경망이 한 샘플에 대해 얼마나 잘 예측했는지 측정하기 위한 것, 신경망의 출력이 기대하는 것보다 얼마나 벗어 났지는지를 측정

**목적함수****-** 더 일반적인 용어로 최적화하기 위한 대상 함수를 의미

**비용함수****(cost Function)-** 모든 훈련 데이터에 대한 손실 함수의 합

**훈련 반복**(training loop)을 통해 충분한 횟수만큼 반복하면 **손실** **함수를 최소화하는 가중치 값을** **산출**



![image-20200203163923338](C:\Users\student\AppData\Roaming\Typora\typora-user-images\image-20200203163923338.png)

 그림 1-9 손실 점수를 피드백 신호로 사용하여 가중치 조정



```
model.compile(loss = 'mse',optimizer='adam', metrics=['mse'])
```

1-1. 손실함수 종류 

평균 제곱 오차, MSE(Mean Squared Error)

교차 엔트로피 오차, CEE(Cross Entropy Error) , 기타 등등



1-2. 옵티마이저 - 오차의 최저점을 찾아주는 것

![image-20200203164327200](C:\Users\student\AppData\Roaming\Typora\typora-user-images\image-20200203164327200.png)



역전파(Backpropagation) : 손실함수의 결과를 개선하기 위해서 다시 결과에서부터 가중치를 수정하는 과정 이를 옵티마이저(optimizer)가 담당



### 2. 특성공학

- 초기 학습을 위한 데이터의 변환을 의미 , 전처리라고도 할 수 있다.

**왜 머신 러닝 보다** **딥** **러닝** **?? -** 특성 공학을 자동화하기 때문



![image-20200203164502668](C:\Users\student\AppData\Roaming\Typora\typora-user-images\image-20200203164502668.png)



##### 2-1) 딥러닝이 데이터로부터 학습하는 방법에의 두 가지 중요한 특징

- 층을 거치면서 점진적으로 더 복잡한 표현이 만들어진다.
- 이러한 점진적인 중간 표현이 공동으로 학습된다.



![image-20200203164621745](C:\Users\student\AppData\Roaming\Typora\typora-user-images\image-20200203164621745.png)

##### 2-2) 과대적합 

- 머신 러닝 모델이 훈련 데이터보다 새로운 데이터에서 성능이 낮아지는 경향



![image-20200203164701006](C:\Users\student\AppData\Roaming\Typora\typora-user-images\image-20200203164701006.png)



#### 3. 데이터 표현 

- ndim() - 넘파이 배열의 축 개수를 확인, 텐서의 축 개수를 랭크(rank)라 부른다.



3-1) 스칼라(0D 텐서) 

- 하나의 숫자만 담고 있는 텐서
- ndim : 0
- ex) 1,4,5 각각 하나의 숫자



3-2) 벡터(1D 텐서)

- ndim : 1
- ex) array([1,2,3,4]) : 4차원 벡터 
- [1,2,3,4] 통째로 하나의 벡터라 칭함 



3-3) 행렬(2D 텐서)

- ndim : 2 

- ex) np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])
- 첫번째 행 : [1,2,3,4] , 첫번째 열:[1,5,9]



3-4) 고차원 텐서

- ndim - 3 이상
- 텐서에서는 차원(dimension)을 축(axis)라고 부른다.
- d=np.**array**([ [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [10, 11, 12, 13, 14]], [[15, 16, 17, 18, 19], [19, 20, 21, 22, 23], [23, 24, 25, 26, 27]] ])



